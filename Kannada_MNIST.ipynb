{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":1,"outputs":[{"output_type":"stream","text":"/kaggle/input/Kannada-MNIST/Dig-MNIST.csv\n/kaggle/input/Kannada-MNIST/train.csv\n/kaggle/input/Kannada-MNIST/sample_submission.csv\n/kaggle/input/Kannada-MNIST/test.csv\n","name":"stdout"}]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils import data\nimport torchvision\n\nimport matplotlib.pyplot as plt","execution_count":2,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/Kannada-MNIST/train.csv').to_numpy()\ntest = pd.read_csv('../input/Kannada-MNIST/test.csv').to_numpy()\n\nX_train, y_train = train[:, 1:], train[:, 0]\nX_test, X_id = test[:, 1:], test[:, 0]\n\nX_train, y_train = torch.tensor(X_train).to(torch.float), torch.tensor(y_train)\nX_test, X_id = torch.tensor(X_test).to(torch.float), torch.tensor(X_id)","execution_count":3,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dset_test = data.TensorDataset(X_test.reshape(-1, 1, 28, 28), X_id)\ndset_train = data.TensorDataset(X_train.reshape(-1, 1, 28, 28), y_train)\n\nfactor = 0.9\ntrain_len = int(len(dset_train) * factor)\nvalid_len = len(dset_train)  - train_len\n\ndset_train, dset_valid = data.random_split(dset_train, [train_len, valid_len])\n","execution_count":4,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(dset_train), len(dset_valid)","execution_count":5,"outputs":[{"output_type":"execute_result","execution_count":5,"data":{"text/plain":"(54000, 6000)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"batch_size = 16\n\ndloader_train = data.DataLoader(dset_train, batch_size=batch_size, shuffle=True)\ndloader_valid = data.DataLoader(dset_valid, batch_size=batch_size, shuffle=True)\ndloader_test = data.DataLoader(dset_test, batch_size=len(dset_train), shuffle=False)","execution_count":6,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(dloader_train), len(dloader_valid)","execution_count":7,"outputs":[{"output_type":"execute_result","execution_count":7,"data":{"text/plain":"(3375, 375)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"x, y = next(iter(dloader_train))","execution_count":8,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.imshow(torchvision.utils.make_grid(x).numpy().transpose(1, 2, 0))\nprint(y)","execution_count":9,"outputs":[{"output_type":"stream","text":"tensor([2, 5, 8, 1, 4, 0, 3, 9, 0, 6, 0, 1, 9, 0, 8, 4])\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Model(nn.Module):\n    '''\n    Convolutional part\n    conv -> relu -> pool -> dropout\n    FC part\n    fc -> relu'''\n    def __init__(self):\n        super(Model, self).__init__()\n        \n        self.conv1 = nn.Conv2d(1, 16, 3, padding=1)   \n        self.conv2 = nn.Conv2d(16, 32, 3,padding=1)\n        self.conv3 = nn.Conv2d(32, 64, 3,padding=1)\n        \n        self.bn1 = nn.BatchNorm2d(16)\n        self.bn2 = nn.BatchNorm2d(32)\n        self.bn3 = nn.BatchNorm2d(64)\n        \n        self.dropout = nn.Dropout(0.25)\n        \n        self.fc1 = nn.Linear(64 * 28**2, 512)\n        self.fc2 = nn.Linear(512, 64)\n        self.fc3 = nn.Linear(64, 10)\n        \n    def forward(self, x):\n        x = F.relu(self.bn1(self.conv1(x)))\n        x = F.relu(self.bn2(self.conv2(x)))\n        x = F.relu(self.bn3(self.conv3(x)))\n\n        x = x.view(x.size(0), -1)\n        \n        x = F.relu(self.dropout(self.fc1(x)))\n        x = F.relu(self.dropout(self.fc2(x)))\n        x = self.fc3(x)\n        return x","execution_count":10,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\nlr = 0.001\nmodel = Model().to(device)\nℓ_fn = nn.CrossEntropyLoss()\noptimizer = torch.optim.SGD(model.parameters(), lr=lr)","execution_count":11,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"epochs = 20\nfor i_epoch in range(1, epochs+1):\n    for x, y in dloader_train:\n        x, y = x.to(device), y.to(device)\n        ŷ = model(x)\n        ℓ = ℓ_fn(ŷ, y)\n        optimizer.zero_grad()\n        ℓ.backward()\n        optimizer.step()\n    model.eval()\n    loss_val = []\n    loss_acc = []\n    for x, y in dloader_train:\n        x, y = x.to(device), y.to(device)\n        ŷ = model(x)\n        ℓ = ℓ_fn(ŷ, y)\n        loss_val.append(ℓ.item())\n        loss_acc.append((y == ŷ.argmax(dim=1)).to(torch.float).mean())\n    model.train()\n    print(f'Epoch {i_epoch} loss: {sum(loss_val)/len(loss_val):.3f}, aacuracy: {sum(loss_acc)/len(loss_acc):.4f}')","execution_count":12,"outputs":[{"output_type":"stream","text":"Epoch 1 loss: 0.093, aacuracy: 0.9747\nEpoch 2 loss: 0.056, aacuracy: 0.9843\nEpoch 3 loss: 0.040, aacuracy: 0.9887\nEpoch 4 loss: 0.032, aacuracy: 0.9908\nEpoch 5 loss: 0.027, aacuracy: 0.9926\nEpoch 6 loss: 0.022, aacuracy: 0.9936\nEpoch 7 loss: 0.018, aacuracy: 0.9948\nEpoch 8 loss: 0.016, aacuracy: 0.9954\nEpoch 9 loss: 0.014, aacuracy: 0.9961\nEpoch 10 loss: 0.012, aacuracy: 0.9964\nEpoch 11 loss: 0.010, aacuracy: 0.9971\nEpoch 12 loss: 0.010, aacuracy: 0.9971\nEpoch 13 loss: 0.009, aacuracy: 0.9975\nEpoch 14 loss: 0.007, aacuracy: 0.9981\nEpoch 15 loss: 0.007, aacuracy: 0.9983\nEpoch 16 loss: 0.006, aacuracy: 0.9983\nEpoch 17 loss: 0.006, aacuracy: 0.9987\nEpoch 18 loss: 0.005, aacuracy: 0.9990\nEpoch 19 loss: 0.004, aacuracy: 0.9993\nEpoch 20 loss: 0.004, aacuracy: 0.9991\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.cpu()\nmodel.eval()\nx, x_id = next(iter(dloader_test))\nŷ = model(x)\nmodel.train()\n\nsubmission  = pd.DataFrame({'id': x_id, 'label': ŷ.argmax(dim=1)})\nsubmission.to_csv('submission.csv',index=False)","execution_count":13,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}